{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e42d78",
   "metadata": {},
   "source": [
    "### Fine Tuning Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b7738",
   "metadata": {},
   "source": [
    "### Importing Libraries :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c735b",
   "metadata": {},
   "source": [
    "Importing required libraries and modules, including NumPy, Hugging Face Transformers, Datasets, pprint, TorchInfo, and the Trainer module from Transformers for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee26cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torchinfo import summary\n",
    "from transformers import Trainer\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9400a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_dataset('rotten_tomatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b72c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bae747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ffe50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_DATASET_REFS',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getitems__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_local_temp_path',\n",
       " '_check_index_is_initialized',\n",
       " '_data',\n",
       " '_estimate_nbytes',\n",
       " '_fingerprint',\n",
       " '_format_columns',\n",
       " '_format_kwargs',\n",
       " '_format_type',\n",
       " '_generate_tables_from_cache_file',\n",
       " '_generate_tables_from_shards',\n",
       " '_get_cache_file_path',\n",
       " '_get_output_signature',\n",
       " '_getitem',\n",
       " '_indexes',\n",
       " '_indices',\n",
       " '_info',\n",
       " '_map_single',\n",
       " '_new_dataset_with_indices',\n",
       " '_output_all_columns',\n",
       " '_push_parquet_shards_to_hub',\n",
       " '_save_to_disk_single',\n",
       " '_select_contiguous',\n",
       " '_select_with_indices_mapping',\n",
       " '_split',\n",
       " 'add_column',\n",
       " 'add_elasticsearch_index',\n",
       " 'add_faiss_index',\n",
       " 'add_faiss_index_from_external_arrays',\n",
       " 'add_item',\n",
       " 'align_labels_with_mapping',\n",
       " 'builder_name',\n",
       " 'cache_files',\n",
       " 'cast',\n",
       " 'cast_column',\n",
       " 'citation',\n",
       " 'class_encode_column',\n",
       " 'cleanup_cache_files',\n",
       " 'column_names',\n",
       " 'config_name',\n",
       " 'data',\n",
       " 'dataset_size',\n",
       " 'description',\n",
       " 'download_checksums',\n",
       " 'download_size',\n",
       " 'drop_index',\n",
       " 'export',\n",
       " 'features',\n",
       " 'filter',\n",
       " 'flatten',\n",
       " 'flatten_indices',\n",
       " 'format',\n",
       " 'formatted_as',\n",
       " 'from_buffer',\n",
       " 'from_csv',\n",
       " 'from_dict',\n",
       " 'from_file',\n",
       " 'from_generator',\n",
       " 'from_json',\n",
       " 'from_list',\n",
       " 'from_pandas',\n",
       " 'from_parquet',\n",
       " 'from_spark',\n",
       " 'from_sql',\n",
       " 'from_text',\n",
       " 'get_index',\n",
       " 'get_nearest_examples',\n",
       " 'get_nearest_examples_batch',\n",
       " 'homepage',\n",
       " 'info',\n",
       " 'is_index_initialized',\n",
       " 'iter',\n",
       " 'license',\n",
       " 'list_indexes',\n",
       " 'load_elasticsearch_index',\n",
       " 'load_faiss_index',\n",
       " 'load_from_disk',\n",
       " 'map',\n",
       " 'num_columns',\n",
       " 'num_rows',\n",
       " 'prepare_for_task',\n",
       " 'push_to_hub',\n",
       " 'remove_columns',\n",
       " 'rename_column',\n",
       " 'rename_columns',\n",
       " 'reset_format',\n",
       " 'save_faiss_index',\n",
       " 'save_to_disk',\n",
       " 'search',\n",
       " 'search_batch',\n",
       " 'select',\n",
       " 'select_columns',\n",
       " 'set_format',\n",
       " 'set_transform',\n",
       " 'shape',\n",
       " 'shard',\n",
       " 'shuffle',\n",
       " 'size_in_bytes',\n",
       " 'sort',\n",
       " 'split',\n",
       " 'supervised_keys',\n",
       " 'task_templates',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_iterable_dataset',\n",
       " 'to_json',\n",
       " 'to_list',\n",
       " 'to_pandas',\n",
       " 'to_parquet',\n",
       " 'to_sql',\n",
       " 'to_tf_dataset',\n",
       " 'train_test_split',\n",
       " 'unique',\n",
       " 'version',\n",
       " 'with_format',\n",
       " 'with_transform']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataframe['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191a4a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': MemoryMappedTable\n",
       " text: string\n",
       " label: int64\n",
       " ----\n",
       " text: [[\"the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\",\"the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\",\"effective but too-tepid biopic\",\"if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\",\"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\",...,\"an uplifting , near-masterpiece .\",\"superior genre storytelling , which gets under our skin simply by crossing the nuclear line .\",\"by taking entertainment tonight subject matter and giving it humor and poignancy , auto focus becomes both gut-bustingly funny and crushingly depressing .\",\"it's a bittersweet and lyrical mix of elements .\",\"subversive , meditative , clinical and poetic , the piano teacher is a daring work of genius .\"],[\"the weakest of the four harry potter books has been transformed into the stronger of the two films by the thinnest of margins .\",\"its gross-out gags and colorful set pieces . . . are of course stultifyingly contrived and too stylized by half . still , it gets the job done -- a sleepy afternoon rental .\",\"it further declares its director , zhang yang of shower , as a boldly experimental , contemporary stylist with a bright future .\",\"smith's approach is never to tease , except gently and in that way that makes us consider our own eccentricities and how they are expressed through our homes .\",\"full of profound , real-life moments that anyone can relate to , it deserves a wide audience .\",...,\"though it's not very well shot or composed or edited , the score is too insistent and the dialogue is frequently overwrought and crudely literal , the film shatters you in waves .\",\"the entire cast is extraordinarily good .\",\"yakusho , as always , is wonderful as the long-faced sad sack . . . and his chemistry with shimizu is very believable .\",\"the film delivers what it promises : a look at the \" wild ride \" that ensues when brash young men set out to conquer the online world with laptops , cell phones and sketchy business plans .\",\"young hanks and fisk , who vaguely resemble their celebrity parents , bring fresh good looks and an ease in front of the camera to the work .\"],...,[\"it won't be long before you'll spy i spy at a video store near you .\",\"this film looks like it was produced in 1954 , shelved for 48 years , and repackaged for a 2002 audience .\",\"propelled not by characters but by caricatures .\",\"there is not an ounce of honesty in the entire production .\",\"this extremely unfunny film clocks in at 80 minutes , but feels twice as long .\",...,\"the slapstick is labored , and the bigger setpieces flat .\",\"this is the kind of movie where people who have never picked a lock do so easily after a few tries and become expert fighters after a few weeks .\",\"the problem with the mayhem in formula 51 is not that it's offensive , but that it's boring .\",\"much of the digitally altered footage appears jagged , as if filmed directly from a television monitor , while the extensive use of stock footage quickly becomes a tiresome cliché .\",\"the film never rises above a conventional , two dimension tale\"],[\"mark wahlberg . . . may look classy in a '60s-homage pokepie hat , but as a character he's dry , dry , dry .\",\"told in scattered fashion , the movie only intermittently lives up to the stories and faces and music of the men who are its subject .\",\"the irony is that this film's cast is uniformly superb ; their performances could have -- should have -- been allowed to stand on their own .\",\"now i can see why people thought i was too hard on \" the mothman prophecies \" .\",\"if ever a concept came handed down from the movie gods on a silver platter , this is it . if ever such a dependable concept was botched in execution , this is it .\",...,\"any enjoyment will be hinge from a personal threshold of watching sad but endearing characters do extremely unconventional things .\",\"if legendary shlockmeister ed wood had ever made a movie about a vampire , it probably would look a lot like this alarming production , adapted from anne rice's novel the vampire chronicles .\",\"hardly a nuanced portrait of a young woman's breakdown , the film nevertheless works up a few scares .\",\"interminably bleak , to say nothing of boring .\",\"things really get weird , though not particularly scary : the movie is all portent and no content .\"]]\n",
       " label: [[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],\n",
       " 'validation': MemoryMappedTable\n",
       " text: string\n",
       " label: int64\n",
       " ----\n",
       " text: [[\"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\"the soundtrack alone is worth the price of admission .\",\"rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\",\"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\",\"bielinsky is a filmmaker of impressive talent .\",...,\"the movie does such an excellent job of critiquing itself at every faltering half-step of its development that criticizing feels more like commiserating .\",\"i found it slow , predictable and not very amusing .\",\"director yu seems far more interested in gross-out humor than in showing us well-thought stunts or a car chase that we haven't seen 10 , 000 times .\",\"viewers will need all the luck they can muster just figuring out who's who in this pretentious mess .\",\"a pint-sized 'goodfellas' designed to appeal to the younger set , it's not a very good movie in any objective sense , but it does mostly hold one's interest .\"],[\"get out your pooper-scoopers .\",\"while the material is slight , the movie is better than you might think .\",\"it's definitely not made for kids or their parents , for that matter , and i think even fans of sandler's comic taste may find it uninteresting .\",\"sheridan seems terrified of the book's irreverent energy , and scotches most of its élan , humor , bile , and irony .\",\"more busy than exciting , more frantic than involving , more chaotic than entertaining .\",...,\"this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .\",\"hilarious musical comedy though stymied by accents thick as mud .\",\"if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .\",\"a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .\",\"the feature-length stretch . . . strains the show's concept .\"]]\n",
       " label: [[1,1,1,1,1,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],\n",
       " 'test': MemoryMappedTable\n",
       " text: string\n",
       " label: int64\n",
       " ----\n",
       " text: [[\"lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\",\"consistently clever and suspenseful .\",\"it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\",\"the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\",\"red dragon \" never cuts corners .\",...,\"the film is strictly routine .\",\"a real snooze .\",\"skillful as he is , mr . shyamalan is undone by his pretensions .\",\"while the new film is much more eye-catching than its blood-drenched stephen norrington-directed predecessor , the new script by the returning david s . goyer is much sillier .\",\"in addition to sporting one of the worst titles in recent cinematic history , ballistic : ecks vs . sever also features terrible , banal dialogue ; convenient , hole-ridden plotting ; superficial characters and a rather dull , unimaginative car chase .\"],[\"it shares the first two films' loose-jointed structure , but laugh-out-loud bits are few and far between .\",\"the santa clause 2 is a barely adequate babysitter for older kids , but i've got to give it thumbs down .\",\"you cannot guess why the cast and crew didn't sign a pact to burn the negative and the script and pretend the whole thing never existed .\",\"barney throws away the goodwill the first half of his movie generates by orchestrating a finale that is impenetrable and dull .\",\"if you're really renting this you're not interested in discretion in your entertainment choices , you're interested in anne geddes , john grisham , and thomas kincaid .\",...,\"a terrible movie that some people will nevertheless find moving .\",\"there are many definitions of 'time waster' but this movie must surely be one of them .\",\"as it stands , crocodile hunter has the hurried , badly cobbled look of the 1959 godzilla , which combined scenes of a japanese monster flick with canned shots of raymond burr commenting on the monster's path of destruction .\",\"the thing looks like a made-for-home-video quickie .\",\"enigma is well-made , but it's just too dry and too placid .\"]]\n",
       " label: [[1,1,1,1,1,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aba69a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a60060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba7945",
   "metadata": {},
   "source": [
    "Loading a tokenizer for the 'distilbert-base-uncased' model using the Hugging Face Transformers library. The tokenizer is created with `AutoTokenizer.from_pretrained(checkpoint)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda36793",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e0d10",
   "metadata": {},
   "source": [
    "Tokenizing the text content of the first three rows in the 'text' column of the 'train' subset of the dataframe using the tokenizer and printing the tokenized result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415049a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1],\n",
      "                    [1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1,\n",
      "                     1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101,\n",
      "                1996,\n",
      "                2600,\n",
      "                2003,\n",
      "                16036,\n",
      "                2000,\n",
      "                2022,\n",
      "                1996,\n",
      "                7398,\n",
      "                2301,\n",
      "                1005,\n",
      "                1055,\n",
      "                2047,\n",
      "                1000,\n",
      "                16608,\n",
      "                1000,\n",
      "                1998,\n",
      "                2008,\n",
      "                2002,\n",
      "                1005,\n",
      "                1055,\n",
      "                2183,\n",
      "                2000,\n",
      "                2191,\n",
      "                1037,\n",
      "                17624,\n",
      "                2130,\n",
      "                3618,\n",
      "                2084,\n",
      "                7779,\n",
      "                29058,\n",
      "                8625,\n",
      "                13327,\n",
      "                1010,\n",
      "                3744,\n",
      "                1011,\n",
      "                18856,\n",
      "                19513,\n",
      "                3158,\n",
      "                5477,\n",
      "                4168,\n",
      "                2030,\n",
      "                7112,\n",
      "                16562,\n",
      "                2140,\n",
      "                1012,\n",
      "                102],\n",
      "               [101,\n",
      "                1996,\n",
      "                9882,\n",
      "                2135,\n",
      "                9603,\n",
      "                13633,\n",
      "                1997,\n",
      "                1000,\n",
      "                1996,\n",
      "                2935,\n",
      "                1997,\n",
      "                1996,\n",
      "                7635,\n",
      "                1000,\n",
      "                11544,\n",
      "                2003,\n",
      "                2061,\n",
      "                4121,\n",
      "                2008,\n",
      "                1037,\n",
      "                5930,\n",
      "                1997,\n",
      "                2616,\n",
      "                3685,\n",
      "                23613,\n",
      "                6235,\n",
      "                2522,\n",
      "                1011,\n",
      "                3213,\n",
      "                1013,\n",
      "                2472,\n",
      "                2848,\n",
      "                4027,\n",
      "                1005,\n",
      "                1055,\n",
      "                4423,\n",
      "                4432,\n",
      "                1997,\n",
      "                1046,\n",
      "                1012,\n",
      "                1054,\n",
      "                1012,\n",
      "                1054,\n",
      "                1012,\n",
      "                23602,\n",
      "                1005,\n",
      "                1055,\n",
      "                2690,\n",
      "                1011,\n",
      "                3011,\n",
      "                1012,\n",
      "                102],\n",
      "               [101, 4621, 2021, 2205, 1011, 8915, 23267, 16012, 24330, 102]]}\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenized = tokenizer(dataframe['train'][0:3]['text'])\n",
    "\n",
    "pprint(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc68d0f",
   "metadata": {},
   "source": [
    "Defining a `tokenize_function` that tokenizes text in batches from a dataframe using the provided tokenizer with truncation enabled. Applying this function to a dataframe (`dataframe`) using the `map` method with batch processing (`batched=True`), and storing the tokenized result in `tokenized_dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd49866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b8c13305914d59a571266a4a5c03a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['text'], truncation=True)\n",
    "\n",
    "tokenized_dataframe = dataframe.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c246d0",
   "metadata": {},
   "source": [
    "Creating a `TrainingArguments` object named `training_args` with the output directory set to 'trainer_log', evaluation strategy set to 'epoch', save strategy set to 'epoch', and training for one epoch (`num_train_epochs = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ce0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    'trainer_log', \n",
    "    evaluation_strategy = 'epoch', \n",
    "    save_strategy = 'epoch', \n",
    "    num_train_epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367519ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels =  2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2aa1f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4275ef5",
   "metadata": {},
   "source": [
    "Iterating through the named parameters of a PyTorch model, `model`, and storing their detached numpy values in the list `params_before`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f827cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_before = []\n",
    "for name, p in model.named_parameters():\n",
    "    params_before.append(p.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a4e4f",
   "metadata": {},
   "source": [
    "Defining a `metric_function` to calculate accuracy between predictions and references using `accuracy_score`. Creating a `compute_metrics` function that computes accuracy based on logits and labels, and configuring a `Trainer` instance with training and evaluation datasets, tokenizer, and metric computation. Initiating training using `trainer.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd30eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1067' max='1067' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1067/1067 1:17:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.530020</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1067, training_loss=0.2381622936531217, metrics={'train_runtime': 4656.3906, 'train_samples_per_second': 1.832, 'train_steps_per_second': 0.229, 'total_flos': 97956536601456.0, 'train_loss': 0.2381622936531217, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def metric_function(predictions, references):\n",
    "    return accuracy_score(references, predictions)\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    training_args, \n",
    "    train_dataset=tokenized_dataframe['train'], \n",
    "    eval_dataset=tokenized_dataframe['validation'], \n",
    "    tokenizer=tokenizer, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a74c3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da3294a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model = 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d644c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9894116520881653}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('That movie was fucking awesome.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb3bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
